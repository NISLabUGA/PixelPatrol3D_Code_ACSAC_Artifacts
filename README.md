# PixelPatrol3D Artifact Submission

This artifact accompanies the paper "PP3D: An In-Browser Vision-Based Defense Against Web Behavior Manipulation Attacks" submitted to ACSAC 2025.

## Overview

This artifact provides reproducible implementations and evaluations for all research questions presented in the paper. The artifact includes pre-trained models, evaluation scripts, and expected results for each research question.

**Main Repository**: For the complete PixelPatrol3D framework implementation, including PP_discover and PP_defend components, please visit our main repository at: https://github.com/NISLabUGA/PixelPatrol3D_Code

### Focus on PP_detect Evaluation

This artifact focuses specifically on evaluating **PP_detect**, which is the main contribution of this work and the integral part of PixelPatrol3D. This focus is justified for several reasons:

1. **Core Technical Contribution**: PP_detect contains the novel machine learning algorithms and multimodal fusion techniques that represent the primary scientific contribution of this work.
2. **Reproducible Evaluation**: The PP_detect evaluation provides standardized, objective metrics that can be independently verified and reproduced by reviewers.
3. **Scientific Rigor**: The evaluation methodology follows established machine learning best practices with proper train/test splits, cross-validation, and statistical significance testing.
4. **Resource Efficiency**: Focusing on PP_detect allows reviewers to evaluate the core claims within reasonable time and computational constraints.

We do not provide explicit test cases for the following components:

**PixelPatrol Defend Evaluation**: We do not provide test cases here because it would be impossible to recreate the test exactly given the hardware requirements and nature of using a personal computer. Additionally, PP_defend is primarily a deployment mechanism rather than an algorithmic contribution. However, the browser extension itself can easily be tested if desired by using the extension on a local machine or mobile device. Instructions for installation and testing can be found in our main repository.

**Data Collection using PixelPatrol Discover**: We do not provide test cases here because: (1) this is not the main contribution of this work, (2) this would take an extremely long time as the data collection runs last for several weeks at a time, and (3) data collection methodology is well-established and does not represent a novel algorithmic contribution. However, if reviewers are interested in how to run data collection, this is outlined in detail in our main repository.

**Text Data Augmentation**: We do not provide test cases for text data augmentation because the models used require API keys from external services, which is not feasible in an artifact evaluation setting. However, we do provide the data generated by these augmentation processes for public use and review, allowing reviewers to examine the quality and characteristics of the augmented text data without needing to reproduce the generation process.

**Model Training from Scratch**: While we provide pre-trained models for all evaluations, we do not include test cases for complete model training from scratch due to the substantial computational requirements (multiple days on high-end GPUs) and the focus on evaluation reproducibility rather than training reproducibility. The training code and hyperparameters are available in the main repository for interested researchers.

## Research Questions Addressed

1. **RQ1 & RQ4**: Can PP_det accurately identify new instances of BMAs belonging to previously observed campaigns and fresh BMA attacks collected well after training data?
2. **RQ2**: Can PP_det accurately identify instances of BMAs captured on a new screen size never seen during training?
3. **RQ3**: Can PP_det identify web pages belonging to never-before-seen BMA campaigns?
4. **RQ5**: Can PP_det be strengthened against adversarial examples?

## System Requirements

### Hardware Requirements

- **Minimum**: 8GB RAM, 4 CPU cores
- **Recommended**: 16GB+ RAM, 8+ CPU cores, NVIDIA GPU with 8GB+ VRAM
- **Storage**: ~500GB free space for datasets and results
  - Dataset: ~218GB compressed, ~270GB uncompressed
  - Additional space needed for temporary files and results

### Software Requirements

- Python 3.8 or higher
- CUDA 11.0+ (optional but recommended for GPU acceleration)
- Linux or macOS (Windows may work but is untested)
- **Recommended**: Ubuntu 22.04 LTS (extensively tested environment)
- **Docker**: Recommended for reproducible execution (see `infrastructure/README.md`)

### Dependencies

All Python dependencies are listed in requirements.txt and will be installed automatically.

**Note**: The `install.sh` script has been extensively tested on Ubuntu 22.04. For maximum reproducibility, consider using Docker with the `ubuntu:22.04` image as detailed in the infrastructure documentation.

## Quick Start

1. **Installation**:

   ```bash
   bash install.sh
   ```

2. **Download Dataset** (Required - ~218GB download):

   ```bash
   python3 download_data.py
   ```

   **Note**: The dataset download is substantial (~218GB compressed, ~270GB uncompressed) and may take several hours. The script will verify checksums and extract files automatically. Use `python3 download_data.py --help` for additional options.

3. **Run a specific claim**:

   ```bash
   cd claims/claim1
   bash run.sh
   ```

4. **Check results**:

   ```bash
   cd claims/claim1/expected
   cat README.md
   ```

## Artifact Structure

```
PixelPatrol3D_Code_ACSAC_Artifacts/
├── install.sh                 # One-click setup script
├── download_data.py           # Dataset download script
├── README.txt                 # This file
├── license.txt                # License information
├── use.txt                    # Intended use and limitations
├── requirements.txt           # Python dependencies
├── artifacts/                 # Core implementation
│   ├── models/               # Pre-trained models for each RQ
│   ├── train_test/           # Training and evaluation scripts
│   ├── pp3d_data/           # Dataset (placeholder - see data README)
│   └── utils/               # Utility scripts
├── infrastructure/           # Public infrastructure compatibility
│   └── README.md            # Public cloud setup and execution guide
└── claims/                   # Reproducibility claims
    ├── claim1/              # RQ1 & RQ4
    ├── claim2/              # RQ2
    ├── claim3/              # RQ3
    └── claim4/              # RQ5
```

## Reproducibility Claims

Each claim directory contains:

- `claim.txt`: Description of the research question and expected results
- `run.sh`: Script to execute the evaluation
- `expected/README.md`: Detailed expected results and validation criteria

### Claim 1: RQ1 & RQ4 (New BMA Instances and Fresh Attacks)

- **Runtime**: ~30 minutes
- **Expected**: >99% detection rate for RQ1, >97% for RQ4

### Claim 2: RQ2 (New Screen Resolutions)

- **Runtime**: ~4 hours (9 leave-one-out cycles)
- **Expected**: >99% detection rate across all resolutions

### Claim 3: RQ3 (Never-before-seen Campaigns)

- **Runtime**: ~6 hours (10 leave-one-out cycles)
- **Expected**: >99% detection rate across all campaigns

### Claim 4: RQ5 (Adversarial Robustness)

- **Runtime**: ~1 hour (2 model evaluations)
- **Expected**: Substantial robustness improvement with adversarial training

### Claim 5: Training Verification - RQ1 & RQ4 Model

- **Runtime**: ~2-3 hours on GPU (limited epochs for verification)
- **Purpose**: Verify training process and demonstrate epoch 4 selection
- **Expected**: Training pipeline functions correctly, model checkpoints created

### Claim 6: Training Verification - RQ5 Adversarial Model

- **Runtime**: ~3-4 hours on GPU (limited epochs for verification)
- **Purpose**: Verify adversarial training process and demonstrate epoch 7 selection
- **Expected**: Adversarial training improves robustness while maintaining clean performance

### Claim 7: Training Verification - RQ2 Resolution Models

- **Runtime**: ~4-6 hours on GPU (limited epochs, subset of resolutions)
- **Purpose**: Verify leave-one-out resolution training and demonstrate epoch selection
- **Expected**: Resolution-agnostic training enables cross-resolution generalization

### Claim 8: Training Verification - RQ3 Campaign Models

- **Runtime**: ~5-7 hours on GPU (limited epochs, subset of campaigns)
- **Purpose**: Verify leave-one-out campaign training and demonstrate epoch selection
- **Expected**: Campaign-agnostic training enables cross-campaign generalization

## Expected Runtime

- **Total evaluation time**: ~12 hours for all claims
- **Individual claims**: 30 minutes to 6 hours each
- **Parallel execution**: Claims can be run independently

## Key Results

The artifact demonstrates:

1. High detection rates (>97%) across all evaluation scenarios
2. Strong generalization to unseen screen resolutions and campaigns
3. Effective adversarial training for robustness improvement
4. Practical deployment feasibility with reasonable computational requirements

## Troubleshooting

### Common Issues

1. **CUDA out of memory**: Reduce batch size in Python scripts
2. **Missing dependencies**: Re-run `bash install.sh`
3. **Slow execution**: Ensure CUDA is properly installed and detected

### Getting Help

For issues with this artifact:

1. Check the expected results in each claim's README
2. Verify system requirements are met
3. Ensure all dependencies are properly installed

## Citation

If you use this artifact, please cite:

```
@inproceedings{pixelpatrol3d2025,
  title={PP3D: An In-Browser Vision-Based Defense Against Web Behavior Manipulation Attacks},
  author={[Authors]},
  booktitle={Annual Computer Security Applications Conference (ACSAC)},
  year={2025}
}
```

## Contact

For questions about this artifact, please contact the paper authors.
