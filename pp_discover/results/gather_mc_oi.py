"""
Meta-Cluster of Interest (MC-OI) Gathering Module

This module identifies and gathers clusters of interest based on social engineering
analysis results. It reads the analysis results from CSV files and copies the
corresponding image clusters to a dedicated directory for further investigation
and manual review.

Key Features:
- Reads cluster analysis results from CSV files generated by tally_se.py
- Identifies clusters flagged as potential BMA attempts
- Copies relevant cluster directories for detailed examination
- Organizes clusters of interest in a structured format for analysis
- Facilitates manual review and validation of automated detection results

The module works with the output of the social engineering detection pipeline:
1. Images are clustered by visual similarity (cluster_phash_hm.py)
2. Clusters are analyzed for social engineering indicators (tally_se.py)
3. This module gathers the flagged clusters for human review

Directory Structure Created:
final/
├── pp_crawler/
│   └── mc_oi/
│       ├── cluster_123/
│       │   ├── image1.png
│       │   └── image2.png
│       └── cluster_456/
│           └── image3.png
└── pp_crawler_baseline/
    └── mc_oi/
        └── cluster_789/
            └── image4.png

This organization allows researchers to:
- Review flagged clusters manually
- Validate automated detection results
- Analyze patterns in social engineering attempts
- Refine detection algorithms based on findings
"""

import os
import shutil
import pandas as pd
import yaml

# Load configuration from YAML file with environment variable expansion
with open('./config.yaml', 'r') as file:
    content = os.path.expandvars(file.read())
    config = yaml.safe_load(content)

def process_directories(sd):
    """
    Process a crawler directory to gather meta-clusters of interest.
    
    This function reads the social engineering analysis results and copies
    the identified clusters of interest to a dedicated directory structure
    for manual review and further analysis.
    
    Args:
        sd (str): Source directory name (crawler type, e.g., 'pp_crawler', 'pp_crawler_baseline')
    """
    print(f"Processing meta-clusters of interest for: {sd}")
    
    # Set up directory paths
    final_dir = config['gather_mc']['dest_base_dir']
    mc_dir = config['clustering']['dest_base_dir']
    crawler_path = os.path.join(final_dir, sd)
    
    print(f"Final directory: {final_dir}")
    print(f"Meta-cluster directory: {mc_dir}")
    print(f"Crawler path: {crawler_path}")
    
    # Ensure the crawler directory exists
    if not os.path.isdir(crawler_path):
        print(f"Crawler directory does not exist: {crawler_path}")
        return
    
    # Look for the analysis results CSV file
    check_csv_path = os.path.join(crawler_path, 'check.csv')
    print(f"Looking for analysis results: {check_csv_path}")
    
    if not os.path.exists(check_csv_path):
        print(f"Analysis results file does not exist: {check_csv_path}")
        print("Make sure tally_se.py has been run first to generate the analysis results.")
        return

    try:
        # Load the CSV containing cluster analysis results
        print("Loading cluster analysis results...")
        df = pd.read_csv(check_csv_path)
        print(f"Loaded {len(df)} cluster analysis records")
        
        # Verify the expected column exists
        if 'clusters' not in df.columns:
            print(f"Error: 'clusters' column not found in {check_csv_path}")
            print(f"Available columns: {list(df.columns)}")
            return
        
        # Create the meta-clusters of interest directory
        mc_oi_dir = os.path.join(crawler_path, "mc_oi")
        os.makedirs(mc_oi_dir, exist_ok=True)
        print(f"Created MC-OI directory: {mc_oi_dir}")
        
        # Process each cluster identified as interesting
        clusters_copied = 0
        clusters_missing = 0
        
        for index, row in df.iterrows():
            cluster_id = row['clusters']
            print(f"Processing cluster: {cluster_id}")
            
            # Locate the source cluster directory
            cluster_dir = os.path.join(mc_dir, sd, str(cluster_id))
            
            if os.path.exists(cluster_dir):
                # Copy the entire cluster directory to MC-OI
                target_dir = os.path.join(mc_oi_dir, str(cluster_id))
                
                try:
                    # Copy the cluster directory (overwrite if exists)
                    if os.path.exists(target_dir):
                        shutil.rmtree(target_dir)
                    shutil.copytree(cluster_dir, target_dir)
                    
                    # Count images in the copied cluster
                    image_count = len([f for f in os.listdir(target_dir) 
                                     if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))])
                    
                    print(f"Successfully copied cluster {cluster_id} ({image_count} images)")
                    clusters_copied += 1
                    
                except Exception as e:
                    print(f"Error copying cluster {cluster_id}: {e}")
            else:
                print(f"Warning: Cluster directory does not exist: {cluster_dir}")
                clusters_missing += 1
        
        # Print summary statistics
        print(f"\nMeta-Cluster Gathering Summary for {sd}:")
        print(f"  Clusters to process: {len(df)}")
        print(f"  Clusters successfully copied: {clusters_copied}")
        print(f"  Clusters missing: {clusters_missing}")
        print(f"  MC-OI directory: {mc_oi_dir}")
        
        if clusters_copied > 0:
            print(f"  Ready for manual review and analysis!")
        
    except pd.errors.EmptyDataError:
        print(f"Error: CSV file is empty: {check_csv_path}")
    except pd.errors.ParserError as e:
        print(f"Error parsing CSV file {check_csv_path}: {e}")
    except Exception as e:
        print(f"Unexpected error processing {check_csv_path}: {e}")

if __name__ == '__main__':
    print("Starting Meta-Cluster of Interest gathering process...")
    
    # Process all crawler directories specified in configuration
    source_dir_list = config['general']['crawler_dir_names']
    for sd in source_dir_list:
        print(f"\n{'='*50}")
        print(f"Processing crawler directory: {sd}")
        print(f"{'='*50}")
        process_directories(sd)
    
    print(f"\n{'='*50}")
    print("Meta-Cluster gathering process complete!")
    print("Clusters of interest are now ready for manual review and analysis.")
    print(f"{'='*50}")
