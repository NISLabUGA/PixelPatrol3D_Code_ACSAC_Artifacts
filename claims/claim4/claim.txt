RQ5: Can PP_det be strengthened against adversarial examples?

This claim addresses Research Question 5 from the paper "PP3D: An In-Browser Vision-Based Defense Against Web Behavior Manipulation Attacks".

RQ5 evaluates whether PP_det can be strengthened against adversarial examples through adversarial training, testing the model's robustness against coordinated multimodal attacks on both visual and textual components.

The experiment compares two models:
1. **Before Adversarial Training**: Model trained on clean data only
2. **After Adversarial Training**: Model trained with adversarial curriculum including PGD attacks

The adversarial evaluation uses:
- **Visual Perturbations**: PGD L-infinity attacks with ε ∈ {2, 4, 8, 16, 32}/255
- **Text Perturbations**: 5 levels of increasing severity (character noise, paraphrasing, semantic inversion)
- **Coordinated Attacks**: Combined visual and textual perturbations at matching levels

Test sets include adversarial examples at 6 different strength levels:
- Level 0: Clean examples (no perturbation)
- Levels 1-5: Increasing adversarial strength

Expected Results:
**Before Adversarial Training**:
- Clean: 100% detection rate at 1% FPR
- Level 4 (ε=16/255): 56.4% detection rate at 1% FPR
- Level 5 (ε=32/255): 4.0% detection rate at 1% FPR

**After Adversarial Training**:
- Clean: 100% detection rate at 1% FPR
- Level 4 (ε=16/255): 98.2% detection rate at 1% FPR
- Level 5 (ε=32/255): 99.4% detection rate at 1% FPR

This demonstrates that adversarial training substantially improves robustness while preserving performance on clean inputs.

## Important Note on Reproducibility

**Reviewers should be aware of expected variability in the "Before Adversarial Training" results:**

The results for the non-adversarially trained model may show higher instability and variance compared to other experiments in this artifact. This is due to two fundamental factors:

1. **Foolbox Framework Variability**: The adversarial example generation relies on the Foolbox library, which introduces some stochasticity in the attack generation process. Different runs may produce slightly different adversarial examples, leading to variation in detection rates.

2. **Decision Boundary Instability**: When a model has not been exposed to adversarial perturbations during training, its decision boundary is inherently unstable against small input changes. This instability is a well-documented phenomenon in adversarial machine learning and is precisely what adversarial training aims to address.

**Expected Variance**: Reviewers may observe detection rates for the non-adversarially trained model that vary by ±5-10% from the reported values, particularly at higher adversarial strength levels (Levels 4-5). This variability is expected and does not indicate experimental error.

**Contrast with Adversarial Training**: The "After Adversarial Training" results should show much more stable and reproducible performance, as adversarial training specifically hardens the model's decision boundaries against such perturbations.

This limitation highlights the importance of adversarial training for achieving both robustness and reproducible performance under adversarial conditions.
